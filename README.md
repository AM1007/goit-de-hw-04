# Homework: Topic ‚ÄúApache Spark. Optimization and SparkUI‚Äù

Hello! üòâ

In this homework, the code is already written for you! However, that might make the assignment even trickier. Your task involves running multiple versions of similar code and analyzing the SparkUI interface.

Here's what you need to do:

1. Run three different programs.
2. Take screenshots of the three sets of Jobs.
3. Analyze and justify the number of Jobs in each set.
4. Understand what the cache function does and why it‚Äôs used.

### Step-by-Step Instructions

**Part 1**
We‚Äôll use the familiar code as a base and add an intermediate operation:

Result of code in task_01.py:
![Result of code in task_01](./screenshots/task_01.png)

### Part 2
We‚Äôll add an intermediate action, collect:

Result of code in task_02.py:
![Result of code in task_02](./screenshots/task_02.png)

> üß† Think: Why does adding just one intermediate action result in 3 more Jobs?

### Part 3
We‚Äôll introduce a new function: `cache`.

> ‚òùüèª The cache() function in PySpark is used to store (or ‚Äúcache‚Äù) data from an RDD (Resilient Distributed Dataset) or DataFrame in memory. This helps speed up subsequent actions or transformations applied to the same data. Caching is especially useful when multiple operations are performed on the same RDD or DataFrame because PySpark won‚Äôt need to recompute the data each time.

**How cache() works:**

1. **Memory caching**: When you call cache() on an RDD or DataFrame, the data is stored in memory (RAM) in a distributed format across all cluster nodes. This improves the performance of future calculations by avoiding repeated data loading or computation.
2. **Lazy execution**: Calling cache() doesn‚Äôt trigger immediate computation. Only when an action (like count(), collect(), or show()) is executed will the data be computed and cached.
3. **Storage mechanism**: By default, cache() uses memory (MEMORY_ONLY). If the data doesn‚Äôt fit in memory, Spark writes it to disk.
4. **Cache control**: By default, cached data is stored with the MEMORY_ONLY storage level. To use other storage levels like MEMORY_AND_DISK, you can use the persist() method instead.

> ‚òùüèª No need to dive deep into these technicalities during this practical assignment. The key takeaway is that data can be stored either in memory or on disk, with memory being the far more common option, while disk storage is rare and exotic üòâ.

```python
from pyspark.sql import SparkSession

# –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ—Å—ñ—é Spark
spark = SparkSession.builder \
    .master("local[*]") \
    .config("spark.sql.shuffle.partitions", "2") \
    .appName("MyGoitSparkSandbox") \
    .getOrCreate()

# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç
nuek_df = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv('./nuek-vuh3.csv')

nuek_repart = nuek_df.repartition(2)

nuek_processed_cached = nuek_repart \
    .where("final_priority < 3") \
    .select("unit_id", "final_priority") \
    .groupBy("unit_id") \
    .count() \
    .cache()  # –î–æ–¥–∞–Ω–æ —Ñ—É–Ω–∫—Ü—ñ—é cache

# –ü—Ä–æ–º—ñ–∂–Ω–∏–π action: collect
nuek_processed_cached.collect()

# –û—Å—å –¢–£–¢ –¥–æ–¥–∞–Ω–æ —Ä—è–¥–æ–∫
nuek_processed = nuek_processed_cached.where("count>2")

nuek_processed.collect()

input("Press Enter to continue...5")

# –ó–≤—ñ–ª—å–Ω—è—î–º–æ –ø—è–º'—è—Ç—å –≤—ñ–¥ Dataframe
nuek_processed_cached.unpersist()

# –ó–∞–∫—Ä–∏–≤–∞—î–º–æ —Å–µ—Å—ñ—é Spark
spark.stop()

```

1. Run the code using cache() on an intermediate result.
2. Take a screenshot of all Jobs (there should be 7).

> üß† Think: Why does using `cache()` reduce the number of Jobs?
